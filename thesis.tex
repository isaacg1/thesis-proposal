\documentclass[11pt]{article}
\usepackage{geometry}
\geometry{margin=1in}
\title{Thesis Proposal}
\author{Isaac Grosof}
\usepackage{multicol}
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}
    {\begin{multicols}{2}[\section*{\refname}]}{}{}
    \patchcmd{\endthebibliography}{\endlist}{\endlist\end{multicols}}{}{}
\date{}
\begin{document}
\maketitle
\section{Introduction}
I research the area of analytical performance modeling and optimization of computer systems,
with a particular focus on scheduling algorithms and resource allocation algorithms.
I study how to optimally schedule jobs,
in settings where job requirements (sizes),
including CPU, network and memory requirements,
are highly variable.
Job sizes may be known in advance,
roughly estimated,
or completely unknown.

Within such settings,
the SIGMETRICS/Performance community has developed a strong understanding
of how to optimally schedule jobs in a single-server system,
when optimizing for mean job delay.
The classic result in this area \cite{doi:10.1287/opre.16.3.687}
says that in a single-server system with \textit{known job sizes},
one should prioritize the shortest jobs to minimize mean job delay.
Specifically, the result states that the optimal scheduling policy in this setting
is the Shortest-Remaining-Processing-Time first (SRPT) policy.
Beyond optimality results,
we can derive the exact mean job delay for a wide variety of scheduling policies,
including the SRPT policy.
More recent optimality results also exist in the single-server, mean-job-delay setting
for unknown or approximately known job sizes \cite{doi:10.1111/j.2517-6161.1979.tb01068.x},
as well as derivations of mean job delay in this setting \cite{Scully:2018:SOC:3203302.3179419}.

However, almost nothing is known outside of the single-server, mean-job-delay setting.
In particular, almost nothing is known about optimal scheduling in multiserver systems,
which consist of multiple servers and some routing mechanism for delivering jobs to servers.
Likewise, almost nothing is known for job-delay objectives other than mean job delay,
including tail objectives such as the 99th percentile of job delay
or the probability that job delay is less than some threshold $t$.
My goal in this thesis is to make progress towards understanding optimal scheduling
in multiserver systems,
and optimal scheduling for the tail of job delay.

The multiserver setting is far more difficult to understand than the single-server setting,
for multiple reasons.
First, the state-space of possible system conditions grows exponentially
with the number of servers,
turning a one-dimensional question into a highly-multidimensional question.
As a result, many tools developed for analyzing single-server systems
become intractable to use on multiserver systems.
Furthermore, multiserver systems are not work-conserving;
there are times when there are fewer jobs present in the system than the number of servers,
leading to not all servers being fully utilized.
In contrast, most commonly studied single-server systems are work-conserving,
and a host of tools have been developed to specifically analyze work-conserving systems.
To understand multiserver systems, new analytical tools must be developed.

Objectives that focus on the tail of job delay are also far more difficult to understand
than the traditional objective of mean job delay.
To analyze and optimize mean job delay,
it suffices to use a ``tagged job analysis'' \cite{harchol2013performance},
where an arriving job need only consider the aggregate total work in the system.
By contrast, when studying the tail of job delay, we cannot simply aggregate jobs.
A job's contribution to the tail changes as the job spends longer in the system --
a job becomes more relevant to the tail as it spends longer in the system.
Thus we need to track the time in system of each job individually.

As a result of these difficulties,
the questions of optimal scheduling in multiserver systems
and optimal scheduling for tail objectives have remained open.
Nonetheless, these questions are the ones we really want to answer for use in the real world.
Real systems feature multiple servers by default.
Computers have multiple processors,
data centers have thousands of machines,
and cloud systems operate at an even larger scale.
Likewise, performance guarantees for latency-sensitive applications
will typically focus on the tail of job delay, not the mean.

This thesis will attempt to make progress on both goals:
Understanding optimal scheduling in multiserver systems
and optimal scheduling for the tail of job delay.

\section{Results so far: Multiserver Systems}

In the area of multiserver systems,
most state-of-the-art results center around the First-Come-First-Served (FCFS) scheduling policy.
Even within FCFS scheduling, surprisingly little is known from the perspective of exact analysis
\cite{Gupta2010} \cite{Gupta2011}.
More precise results have only been derived using numerical methods,
not with symbolic or analytical methods.
Basic bounds on mean job delay based on the job size distribution
have long been known \cite{doi:10.1111/j.2517-6161.1970.tb00819.x}.
More recently, tighter bounds have been derived \cite{daley1997some} \cite{Gupta:2011:TMB:1993744.1993792}.

Despite the heavy focus on the FCFS policy, FCFS scheduling is nowhere near optimal.
Instead, we want to study the SRPT scheduling policy for multiserver systems.
The SRPT scheduling policy is optimal for single-server systems,
and it stands to reason that SRPT scheduling might be at least near optimal
for multiserver systems.
Unfortunately, no analytical results exist on SRPT scheduling in multiserver systems.
I decided to attack this problem, towards my goal of optimal scheduling in multiserver systems.

In my paper in IFIP Performance 2018 \cite{GROSOF2018154},
I derived the first upper bound on mean job delay for SRPT scheduling in multiserver systems.
This is the first result of its kind for any non-trivial multiserver scheduling policy.
My bound applies to any multiserver system
where jobs arrive according to a Poisson process
and job sizes are drawn i.i.d. from a (nearly) arbitrary job size distribution.
In particular, my bound is tight enough to prove that the SRPT scheduling policy
is optimal for multiserver systems,
in the limit as system load approaches capacity.
This is the first optimality result known for any scheduling policy in a multiserver system.
Moreover, my technique is general enough to prove bounds on several other scheduling policies,
including a policy where job sizes are unknown.
This paper won the Best Student Paper Award at IFIP Performance 2018,
one of the two premier conferences in the performance modeling field.

My next goal was to make these multiserver results more practical for use in real systems.
The \cite{GROSOF2018154} paper focused on a multiserver system with a \textit{central queue},
where jobs are buffered in a central queue until a server becomes available for them to run on,
and where each job can be preempted and migrated to another server with no cost or delay.
Next, I turned to a \textit{dispatching}-based model of a multiserver system.
In many real systems, one often has a front-end dispatcher (load balancer),
and once a job is dispatched to a server,
the job is never moved to another server.
Such systems are known as load balancing systems.

In the area of load balancing systems,
most state-of-the-art results use the FCFS scheduling policy at the servers,
and study any of a wide variety of dispatching policies.
Dispatching policies that have been heavily analyzed include Join-the-Shortest-Queue (JSQ)
\cite{doi:10.1287/moor.2017.0887} \cite{doi:10.1287/opre.34.1.55} \cite{491583},
Power-of-d-Queues
\cite{Bramson:2010:RLB:1811099.1811071} \cite{vvedenskaya1996queueing} \cite{963420},
Threshold dispatching policies
\cite{Teh2002} \cite{Zhou:2017:DLH:3175501.3154498}
and many more.

As with the central-queue multiserver setting,
none of these dispatching policies are anywhere near optimal,
especially in settings with high job size variability.
Instead, we want to study load balancing systems that use SRPT scheduling at the servers.
A simple theorem \cite{Grosof:2019:LBG:3341617.3326157}
shows that an optimal policy for a load balancing system
must include SRPT scheduling at the servers.
Unfortunately, no previous analytical results exist
on load balancing systems using SRPT scheduling at the servers.
I decided to attack this problem,
to push towards my goal of optimal scheduling in multiserver systems.

In my paper in SIGMETRICS/Performance 2019 \cite{Grosof:2019:LBG:3341617.3326157},
I proved the first analytical result on load balancing systems with SRPT scheduling at the servers.
In particular, I invented a novel resource allocation algorithm to dispatch jobs to servers,
because the standard dispatching algorithms from the literature didn’t work well enough.
More than a single algorithm,
I created a general framework for turning a baseline dispatching algorithm
into a provably near-optimal algorithm.
I proved an upper bound on the mean job delay
in a load balancing system using any dispatching algorithm in my framework
in concert with SRPT scheduling at the servers.
This bound is tight enough to prove that any such combined policy
is optimal in the limit as system load approaches capacity.
This is the first optimality result known for any combined policy in a load balancing system.
This paper won the Outstanding Student Paper Award at SIGMETRICS/Performance 2019,
the collocation of the two premier conferences in the performance modeling field.

\section{Future: Tail of Job Delay}
Next, I plan to attack the area of the tail of job delay.
First, I want to study the tail of job delay in single-server systems,
since almost no results exist even in that simpler setting.
Later, I want to study the tail of job delay in multiserver systems,
to bring my results even closer to the real world.

\subsection{Near Future: Single Server Tail}
There are two directions I am exploring for studying the tail of job delay in single-server systems: Bounding the tail of job delay under existing scheduling policies, and developing new optimal or near optimal scheduling policies.

In the area of bounding the tail of job delay,
one useful starting point is the Laplace-Stiljes transform
of the distribution of job delays,
which is already known for many existing scheduling policies \cite{Scully:2018:SOC:3203302.3179419}.
However, it is not simple to convert the transform of job delay
into a standard metric of tail performance such as a percentile of job delay.
The transform expressions are extremely complicated for all but the simplest scheduling policies,
and no symbolic inversion technique is currently known.
Only numerical inversion techniques are currently available,
which do not allow us to prove analytical results.
However, one avenue of potential progress is to weaken our target
from an exact symbolic inverse to symbolic bounds on a percentile of job delay.
I hope to develop symbolic tools to bound the tail of job delay
for simple but nontrivial scheduling policies.

Second, I hope to develop new optimal or near optimal scheduling policies
to minimize metrics of the tail of job delay.
One surprisingly tricky problem in this area is defining the right metric(s)
to characterize the tail performance of a given scheduling policy.
One popular type of tail metric is the percentile of job delay,
such as the 99th percentile.
Another type of metric of interest is the mean of job delays above a given percentile.
Many other tail metrics can be defined,
such as higher moments of job delay,
the fraction of jobs completing under a given amount of time,
or the mean amount by which jobs exceed a soft deadline.
Each of these metrics corresponds to a different real-world setting,
and each incentivizes different policy choices.
Ideally, I want to prove novel results about a variety of different types of tail metrics,
so that scheduling policies can be tailored to the real world objectives.

Once we’ve chosen a metric for measuring the tail of job delay,
we can apply a tool known as interchange arguments
to the design of optimal or near-optimal scheduling policies.
An interchange argument compares the two distinct orderings of a pair of jobs.
Under some objectives,
one can prove that one ordering is always strictly better than the opposite ordering,
regardless of any other events happening in the system.
In the case of the mean job delay objective,
one ordering is always strictly better than the other,
for any pair of jobs.
As a result, we can use interchange arguments to prove that the SRPT scheduling policy is optimal,
for that objective.
Under other objectives,
the strict improvement under one ordering may exist for some pairs of jobs but not for others.
In this case, an interchange argument can narrow
the space of possible scheduling policies to consider,
but cannot uniquely identify an optimal policy.
Furthermore, we can consider more than two jobs at a time to prove yet stronger results.
I hope to use interchange arguments as a starting point in the design of
optimal or near optimal scheduling policies in this setting.

Another analytical tool that I am trying to apply to this setting
is the Multiarmed Bandit (MAB) problem.
The MAB problem is a closely related problem to the question of optimal scheduling,
and MAB has long been exactly solved \cite{doi:10.1111/j.2517-6161.1979.tb01068.x}.
This tool has been applied to scheduling problems under the objective of mean job delay.
This gave rise to the Gittins Index policy,
which is the optimal scheduling policy for mean job delay
given unknown or approximately known job sizes.
Replacing mean job delay with a tail metric for job delay
requires us to switch from the multiarmed bandit problem
to the restless multiarmed bandit problem (RMAB).
While the RMAB problem is not understood to the extent that the MAB problem is understood,
RMAB is understood much better than scheduling for the tail of job delay.
I hope to apply RMAB theory to scheduling in this setting,
and thereby derive optimal or near optimal policies for metrics of the tail of job delay,
even in the presence of unknown or partially known job sizes.

\subsection{Farther Future: Multiserver Tail}
The question of scheduling to optimize the tail of job delay in a multiserver system
is yet more difficult and open than optimizing the tail of job delay in a single server system.
This extra complexity is illustrated by the following observation:
In multiserver systems, long tails can be caused by wasted server capacity,
but too aggressively preventing wasted capacity can also cause long tails.

In the former case, consider a scheduling policy like SRPT
that always prioritizes short jobs.
Under certain conditions, the system may finish all of its small jobs,
leaving only a small number of large jobs slowly running on a couple of servers.
These very large and slow jobs might dominate the tail of job delay.
A better policy might have prioritized those large jobs earlier on,
knowing that they would eventually contribute to the tail of job delay.
By doing so, the system would spend less time with only a few servers occupied
and achieve a better tail of job delay.

In the latter case, consider a scheduling policy like longest remaining processing time (LRPT),
which is known to keep as many servers occupied as possible.
Under this scheduling policy, many small jobs are kept around until the system empties.
This helps keep the servers occupied,
but it also leads to a large number of jobs with a large delay,
hurting the tail of job delay anyways.

To optimize the tail of job delay, I am looking for ways to balance this tradeoff, and avoid major failings in either direction.
\section{Conclusion}
I research the area of analytical performance optimization of computer systems.
In this area, the SIGMETRICS/Performance community has a strong understanding
of optimally scheduling jobs in a single-server system to minimize mean job delay.
However, almost is known about optimal scheduling in multiserver systems
or when the objective focuses on the tail of job delay,
Taking on the multiserver system challenge,
I have proven the first bounds on mean job delay and the first optimality results
in multiserver systems.
My results apply both to central-queue multiserver systems
and to load balancing systems.
Next, I want to take on the tail of job delay.
Starting in the single-server setting,
I hope to apply a variety of analytical tools to bound the tail of job delay for existing policies,
and to develop new optimal or near optimal policies.
After that, I hope to simultaneously tackle the multiserver setting along with
the tail of job delay,
and address the additional tradeoffs that arise in such systems.

\footnotesize
\bibliographystyle{apalike}
\bibliography{thesis}
\end{document}
